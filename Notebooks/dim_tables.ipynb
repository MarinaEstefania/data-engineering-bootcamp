{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dim_tables.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRVZ9qyCRczdNJt+ZzZklU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaEstefania/data-engineering-bootcamp/blob/main/Notebooks/dim_tables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MIfYEr_y1t-f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession, Window\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"review_logs\") \\\n",
        "       .config('spark.jars.packages', 'com.databricks:spark-xml_2.12:0.15.0')\\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract data\n",
        "df = spark.read.parquet(\"sample_data/part-00000-c3039b56-7ffc-4dae-b3da-aaa9da7784c8-c000.snappy.parquet\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpexo-242YLT",
        "outputId": "e7d20bed-3c16-4811-c0df-f77b969aa8bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------------+-----------------+------------+------------+--------------+------+\n",
            "|  log_date|  device|     location|               os|          ip|phone_number|       browser|log_id|\n",
            "+----------+--------+-------------+-----------------+------------+------------+--------------+------+\n",
            "|04-25-2021|  Mobile|       Kansas|        Apple iOS|9.200.232.57|821-540-5777|        Safari|     1|\n",
            "|03-13-2021|  Tablet|       Oregon|   Google Android|9.200.232.57|819-102-1320| Google Chrome|     2|\n",
            "|09-30-2021|  Tablet|    Minnesota|        Apple iOS|9.200.232.57|989-156-0498|        Safari|     3|\n",
            "|05-24-2021|  Tablet|     Arkansas|      Apple MacOS|9.200.232.57|225-837-9935|        Safari|     4|\n",
            "|02-01-2021|  Tablet|New Hampshire|            Linux|9.200.232.57|243-842-4562|       Firefox|     5|\n",
            "|07-23-2021|  Tablet|  Pensylvania|        Apple iOS|9.200.232.57|694-501-4352|        Safari|     6|\n",
            "|10-13-2021|Computer|     New York|      Apple MacOS|9.200.232.57|430-449-7136|        Safari|     7|\n",
            "|09-18-2021|Computer|   California|Microsoft Windows|9.200.232.57|633-661-7714|Microsoft Edge|     8|\n",
            "|05-08-2021|  Tablet|   Washington|      Apple MacOS|9.200.232.57|450-036-0504|        Safari|     9|\n",
            "|03-04-2021|  Tablet|     Lousiana|            Linux|9.200.232.57|738-536-6776|       Firefox|    10|\n",
            "|06-19-2021|Computer|    Minnesota|   Google Android|9.200.232.57|682-519-9021| Google Chrome|    11|\n",
            "|03-23-2021|  Mobile|     Arkansas|   Google Android|9.200.232.57|678-722-6084| Google Chrome|    12|\n",
            "|04-08-2021|  Mobile|  Pensylvania|            Linux|9.200.232.57|781-850-8167|       Firefox|    13|\n",
            "|06-29-2021|  Mobile|     Lousiana|Microsoft Windows|9.200.232.57|208-216-2106|Microsoft Edge|    14|\n",
            "|10-20-2021|Computer|        Idaho|            Linux|9.200.232.57|693-854-2646|       Firefox|    15|\n",
            "|06-03-2021|  Mobile|      Montana|            Linux|9.200.232.57|805-540-1405|       Firefox|    16|\n",
            "|05-27-2021|  Tablet|     Nebraska|      Apple MacOS|9.200.232.57|896-134-1623|        Safari|    17|\n",
            "|04-01-2021|  Tablet|      Alabama|      Apple MacOS|9.200.232.57|850-716-4779|        Safari|    18|\n",
            "|06-12-2021|  Tablet|     Missouri|   Google Android|9.200.232.57|946-565-9757| Google Chrome|    19|\n",
            "|05-09-2021|Computer| Rhode Island|   Google Android|9.200.232.57|938-623-1577| Google Chrome|    20|\n",
            "+----------+--------+-------------+-----------------+------------+------------+--------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform data (create dim dataframes)\n",
        "dim_devices = df.select(col('log_id'),col('device')) \\\n",
        "  .drop_duplicates(['device']) \\\n",
        "  .withColumnRenamed('log_id', 'id_dim_devices')\n",
        "dim_devices.show()\n",
        "\n",
        "dim_location = df.select(col('log_id'),col('location')) \\\n",
        "  .drop_duplicates(['location']) \\\n",
        "  .withColumnRenamed('log_id', 'id_dim_location')\n",
        "dim_location.show()\n",
        "\n",
        "dim_os = df.select(col('log_id'),col('os')) \\\n",
        "  .drop_duplicates(['os']) \\\n",
        "  .withColumnRenamed('log_id', 'id_dim_os')\n",
        "dim_os.show()\n",
        "\n",
        "dim_browser = df.select(col('log_id'),col('browser')) \\\n",
        "  .drop_duplicates(['browser']) \\\n",
        "  .withColumnRenamed('log_id', 'id_dim_browser')\n",
        "dim_browser.show()\n",
        "\n",
        "dim_date = df.select(df.log_id, df.log_date) \\\n",
        "  .drop_duplicates(['log_date']) \\\n",
        "  .withColumnRenamed('log_id', 'id_dim_date')\n",
        "dim_date = dim_date.withColumn('day', split(col('log_date'),'-').getItem(1)) \\\n",
        "  .withColumn('month', split(col('log_date'),'-').getItem(0)) \\\n",
        "  .withColumn('year', split(col('log_date'),'-').getItem(2)) \\\n",
        "  .withColumn('season', lit('under construction...'))\n",
        "dim_date.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltFido3I3-UQ",
        "outputId": "9d4285da-b1c4-4dbf-c147-a178a0a83c28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------+\n",
            "|id_dim_devices|  device|\n",
            "+--------------+--------+\n",
            "|             7|Computer|\n",
            "|             1|  Mobile|\n",
            "|             2|  Tablet|\n",
            "+--------------+--------+\n",
            "\n",
            "+---------------+-------------+\n",
            "|id_dim_location|     location|\n",
            "+---------------+-------------+\n",
            "|             55|         Utah|\n",
            "|             52|       Hawaii|\n",
            "|              3|    Minnesota|\n",
            "|             31|         Ohio|\n",
            "|              2|       Oregon|\n",
            "|              4|     Arkansas|\n",
            "|             24|        Texas|\n",
            "|             90| North Dakota|\n",
            "|             89|  Connecticut|\n",
            "|             17|     Nebraska|\n",
            "|            148|      Vermont|\n",
            "|            114|       Nevada|\n",
            "|              9|   Washington|\n",
            "|             33|     Illinois|\n",
            "|            107|     Oklahoma|\n",
            "|             63|     Delaware|\n",
            "|            177|       Alaska|\n",
            "|             39|   New Mexico|\n",
            "|             25|West Virginia|\n",
            "|             19|     Missouri|\n",
            "+---------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------+-----------------+\n",
            "|id_dim_os|               os|\n",
            "+---------+-----------------+\n",
            "|        8|Microsoft Windows|\n",
            "|        5|            Linux|\n",
            "|        1|        Apple iOS|\n",
            "|        4|      Apple MacOS|\n",
            "|        2|   Google Android|\n",
            "+---------+-----------------+\n",
            "\n",
            "+--------------+--------------+\n",
            "|id_dim_browser|       browser|\n",
            "+--------------+--------------+\n",
            "|             8|Microsoft Edge|\n",
            "|             5|       Firefox|\n",
            "|             1|        Safari|\n",
            "|             2| Google Chrome|\n",
            "+--------------+--------------+\n",
            "\n",
            "+-----------+----------+---+-----+----+--------------------+\n",
            "|id_dim_date|  log_date|day|month|year|              season|\n",
            "+-----------+----------+---+-----+----+--------------------+\n",
            "|        234|05-01-2021| 01|   05|2021|under constructio...|\n",
            "|        227|05-04-2021| 04|   05|2021|under constructio...|\n",
            "|        580|04-06-2021| 06|   04|2021|under constructio...|\n",
            "|        124|08-10-2021| 10|   08|2021|under constructio...|\n",
            "|        326|03-08-2021| 08|   03|2021|under constructio...|\n",
            "+-----------+----------+---+-----+----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "#Load data from AWS Glue Studio to Snowflake DataWarehouse\n",
        "dim_devices.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"dbtable\", \"DIM_DEVICES\").mode(\"overwrite\").save()\n",
        "dim_location.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"dbtable\", \"DIM_LOCATION\").mode(\"overwrite\").save()\n",
        "dim_os.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"dbtable\", \"DIM_OS\").mode(\"overwrite\").save()\n",
        "dim_browser.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"dbtable\", \"DIM_BROWSER\").mode(\"overwrite\").save()\n",
        "dim_date.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option(\"dbtable\", \"DIM_DATE\").mode(\"overwrite\").save()\n",
        "\n",
        "job.commit()\n",
        "#fact_movie_analytics.write.option(\"header\", \"true\").csv(\"s3://manual-bucket-megc/gold-data/dim_devices/\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "L_8MM1MQ4wPa",
        "outputId": "c8563f11-77d1-4d32-deab-2931f4ff0a35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1259e5f83a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdim_devices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://manual-bucket-megc/gold-data/dim_devices/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdim_location\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://manual-bucket-megc/gold-data/dim_location/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdim_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://manual-bucket-megc/gold-data/dim_os/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdim_browser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://manual-bucket-megc/gold-data/dim_browser/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.2-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o100.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    }
  ]
}